#!/usr/bin/env python
# coding: utf-8

# # 1) Write a python program which searches all the product under a particular product from www.amazon.in. The 
# product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for 
# guitars

# In[1]:


get_ipython().system('pip install selenium')


# In[2]:


import selenium
import pandas as pd
from selenium import webdriver
import warnings
warnings.filterwarnings('ignore')
from selenium.webdriver.common.by import By
import time


# In[3]:


driver = webdriver.Chrome()


# In[ ]:


driver.get("https://www.amazon.in/")


# In[ ]:


Input_element=input('Enter the product name------')
search_element = driver.find_element(By.XPATH, "//*[@id='twotabsearchtextbox']" )
search_element.send_keys(Input_element)


# In[ ]:


search_button = driver.find_element(By.ID, "nav-search-submit-button")
search_button.click()


# In[ ]:


product_name=[]


# In[ ]:


PName=driver.find_elements(By.XPATH, "//span[@class='a-size-medium a-color-base a-text-normal']")
for i in PName:
    if i.text is None :
        productName.append("--") 
    else:
        product_name.append(i.text)
print(len(product_name))


# # 2)In the above question, now scrape the following details of each product listed in first 3 pages of your search 
# results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then 
# scrape all the products available under that product name. Details to be scraped are: "Brand 
# Name", "Name of the Product", "Price", "Return/Exchange", "Expected Delivery", "Availability" and 
# “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. 

# In[ ]:


start_page = 0
end_page = 3
urls = []
for page in range(start_page,end_page+1):
    try:
        page_urls = driver.find_elements(By.XPATH,'//a[@class="a-link-normal s-no-outline"]')
        
      
        for url in page_urls:
            url = url.get_attribute('href')    
            if url[0:4]=='http':               
                urls.append(url)                
        print("Product urls of page {} has been scraped.".format(page+1))
        
        # Moving to next page
        nxt_button = driver.find_element(By.XPATH,'//li[@class="a-last"]/a')      
        if nxt_button.text == 'Next':                                          
            nxt_button.click()                                                 
            time.sleep(5)                                                      
           
        elif driver.find_element_by_xpath(By.XPATH,'//li[@class="a-disabled a-last"]/a').text=='Next':    
            print("No new pages exist. Breaking the loop")  
            break
            
    except StaleElementReferenceException as e:                
        print("Stale Exception")
        next_page = nxt_button.get_attribute('href')        
        driver.get(next_page)                               


# In[ ]:


Brand=[]
Name=[]
Rating=[]
No_of_rating=[]
Price=[]
Return/Exchange=[]
Expected_Delovery=[]
Availability=[]
Other_Details=[]
URL=[]


# In[ ]:


for url in urls[:4]:
    driver.get(url)                                                       
    print("Scraping URL = ", url)
    #time.sleep(2)
    
    try:
        brand = driver.find_element_by_xpath('//a[@id="bylineInfo"]')      
        prod_dict['Brand'].append(brand.text)
    except NoSuchElementException:
        prod_dict['Brand'].append('-')
    
    try:
        name = driver.find_element_by_xpath('//h1[@id="title"]/span')      
        prod_dict['Name'].append(name.text)
    except NoSuchElementException:
        prod_dict['Name'].append('-')
    
    try:
        rating = driver.find_element_by_xpath('//span[@id="acrPopover"]')  
        prod_dict['Rating'].append(rating.get_attribute("title"))
    except NoSuchElementException:
        prod_dict['Rating'].append('-')
        
    try:
        n_rating = driver.find_element_by_xpath('//a[@id="acrCustomerReviewLink"]/span')     
        prod_dict['No. of ratings'].append(n_rating.text)
    except NoSuchElementException:
        prod_dict['No. of ratings'].append('-')
    
    try:
        price = driver.find_element_by_xpath('//span[@id="priceblock_ourprice"]')           
        prod_dict['Price'].append(price.text)
    except NoSuchElementException:
        prod_dict['Price'].append('-')
    try:                                                                                     
        ret = driver.find_element_by_xpath('//div[@data-name="RETURNS_POLICY"]/span/div[2]/a')
        prod_dict['Return/Exchange'].append(ret.text)
    except NoSuchElementException:
        prod_dict['Return/Exchange'].append('-')
    try:
        delivry = driver.find_element_by_xpath('//div[@id="ddmDeliveryMessage"]/b')         
        prod_dict['Expected Delivery'].append(delivry.text)
    except NoSuchElementException:
        prod_dict['Expected Delivery'].append('-')
    
    try:
        avl = driver.find_element_by_xpath('//div[@id="availability"]/span')                
        prod_dict['Availability'].append(avl.text)
    except NoSuchElementException:
        prod_dict['Availability'].append('-')
        
    try:                                                                                  
        dtls = driver.find_element_by_xpath('//ul[@class="a-unordered-list a-vertical a-spacing-mini"]')
        prod_dict['Other Details'].append('  ||  '.join(dtls.text.split('\n')))
    except NoSuchElementException:
        prod_dict['Other Details'].append('-')
    
    prod_dict['URL'].append(url)                                                          
    time.sleep(2)


# In[ ]:


df=pd.DataFrame({'Brand':=Brand,'Name':Name,'Rating':Rating,'No_of_rating':NoRating,'Price':Price,'Return/Exchange':Return,'Expected_Delivery':Expected,'Avalibility':Avalibility,'Other_Details':Other,'URL':URL})
df


# In[ ]:


prod_df.to_csv('Amazon_{}.csv'.format(input_element)


# # 3)Write a python program to access the search bar and search button on images.google.com and scrape 10 
# images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’

# In[ ]:


driver = webdriver.Chrome()


# In[ ]:


driver.get('https://images.google.com/')


# In[ ]:


driver.get("https://images.google.com/?gws_rd=ssl")

search_element = driver.find_element(By.CLASS_NAME, "gLFyf" )
search_element.send_keys('Fruits Images')

search_button = driver.find_element(By.CLASS_NAME, "zgAlFc")
search_button.click()



# In[ ]:


for _ in range(500):
    driver.execute_script("window.scrollBy(0,10000)")


# In[ ]:


driver.get("https://images.google.com/?gws_rd=ssl")

search_element = driver.find_element(By.CLASS_NAME, "gLFyf" )
search_element.send_keys('Cars Images')

search_button = driver.find_element(By.CLASS_NAME, "zgAlFc")
search_button.click()


# In[ ]:


driver.get("https://images.google.com/?gws_rd=ssl")

search_element = driver.find_element(By.CLASS_NAME, "gLFyf" )
search_element.send_keys('Machine Learning Images')

search_button = driver.find_element(By.CLASS_NAME, "zgAlFc")
search_button.click()


# In[ ]:


driver.get("https://images.google.com/?gws_rd=ssl")

search_element = driver.find_element(By.CLASS_NAME, "gLFyf" )
search_element.send_keys('Guitar Images')

search_button = driver.find_element(By.CLASS_NAME, "zgAlFc")
search_button.click()


# In[ ]:


driver.get("https://images.google.com/?gws_rd=ssl")

search_element = driver.find_element(By.CLASS_NAME, "gLFyf" )
search_element.send_keys('Cakes Images')

search_button = driver.find_element(By.CLASS_NAME, "zgAlFc")
search_button.click()


# In[ ]:


Fruits=[]
Cars=[]
Machine_Learning=[]
Guitar=[]
Cakes=[]


# In[ ]:


fruits_images=driver.find_elements(By.XPATH, "//div[@class='zbRPDe M2qv4b P4HtKe']")
for i in fruits_images[0:10]:
    fruits=i.text
    Fruits.append(fruits)
    
    

cars_images=driver.find_elements(By.XPATH,"//div[@class='zbRPDe M2qv4b P4HtKe']")
for i in cars_images[0:10]:
    cars=i.text
    Cars.append(cars)
    

machine_images=driver.find_elements(By.XPATH,"//div[@class='zbRPDe M2qv4b P4HtKe']")
for i in machine_images[0:10]:
    machine=i.text
    Machine_Learning.append(machine)
    
    
guitar_images=driver.find_elements(By.XPATH,"//div[@class='zbRPDe M2qv4b P4HtKe']")
for i in guitar_images[0:10]:
    guitar=i.text
    Guitar.append(guitar)
    
cake_images=driver.find_elements(By.XPATH,"//div[@class='zbRPDe M2qv4b P4HtKe']")
for i in cake_images[0:10]:
    cake=i.text
    Cakes.append(cars)
print(len(Fruits),len(Cars),len(Machine_Learning),len(Guitar),len(Cakes))


# In[ ]:


df=pd.DataFrame({'Fruits_Img':Fruits, 'Cars_Img':Cars,'Machine_Img':Machine_Learning, 'Guitar_Img':Guitar,'Cake_Img':Cakes})
df


# # 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com
# and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand 
# Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, 
# “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the 
# details is missing then replace it by “- “. Save your results in a dataframe and CSV

# In[ ]:


driver = webdriver.Chrome()


# In[ ]:


driver.get("https://www.flipkart.com/")


# In[ ]:


search_element = driver.find_element(By.CLASS_NAME, "Pke_EE" )
search_element.send_keys('Oneplus Nord, pixel 4A')


# In[ ]:


search_button = driver.find_element(By.CLASS_NAME, "_2iLD__")
search_button.click()


# In[ ]:


Brand_Name=[]
RAM_ROM=[]
Primary_Camera=[]
Display_Size=[]
Processor=[]
Battery_Capacity=[]
Price=[]


# In[ ]:


brand_name=driver.find_elements(By.XPATH,"//div[@class='_3879cV']")
for i in brand_name:
    if i.text is None:
        Brand_Name.append('--')
    else:
        Brand_Name.append(i.text)
    
print(len(Brand_Name))


# In[ ]:


ram=driver.find_elements(By.XPATH,"//ul[@class='_1xgFaf']")
for i in ram:
    if i.text is None :
        RAM_ROM.append("--") 
    else:
        RAM_ROM.append(i.text)
print(len(RAM_ROM))


# In[ ]:


PC=driver.find_elements(By.XPATH,"//ul[@class='_1xgFaf']")
for i in PC:
    if i.text is None :
        Primary_Camera.append("--") 
    else:
        Primary_Camera.append(i.text)
print(len(Primary_Camera))


# In[ ]:


DS=driver.find_elements(By.XPATH,"//ul[@class='_1xgFaf']")
for i in DS:
    if i.text is None :
        Display_Size.append("--") 
    else:
        Display_Size.append(i.text)
print(len(Display_Size))


# In[ ]:


Processor=driver.find_elements(By.XPATH,"//ul[@class='_1xgFaf']")
for i in Processor:
    if i.text is None :
        Processor.append("--") 
    else:
        Processor.append(i.text)
print(len(Processor))


# In[ ]:


Battery=driver.find_elements(By.XPATH,"//ul[@class='_1xgFaf']")
for i in Battery:
    if i.text is None :
        Battery_Capacity.append("--") 
    else:
        Battery_Capacity.append(i.text)
print(len(Battery))


# In[ ]:


price=driver.find_elements(By.XPATH,"//div[@class='_30jeq3 _1_WHN1']")
for i in price:
    if i.text is None :
        Price.append("--") 
    else:
        Price.append(i.text)
print(len(Price))


# In[ ]:


df=pd.DataFrame({'Brand':Brand_Name,'Memory':RAM_ROM,'Camera':Primary_Camera,'Display':Display_Size,'Processor':Processor,'Battery':Battery_Capacity,'Price':Price})
df


# # 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.

# In[ ]:


driver=webdriver.Chrome()


# In[ ]:


import regex as re


# In[ ]:




driver.get("https://www.google.co.in/maps")
time.sleep(3)

city = input('Enter City Name : ')                                         
search = driver.find_element(By.ID,"searchboxinput")                       
search.clear()                                                           
time.sleep(2)
search.send_keys(city)                                                    
button = driver.find_element(By.ID,"searchbox-searchbutton")              
button.click()                                                            
time.sleep(3)

try:
    url_string = driver.current_url
    print("URL Extracted: ", url_string)
    lat_lng = re.findall(r'@(.*)data',url_string)
    if len(lat_lng):
        lat_lng_list = lat_lng[0].split(",")
        if len(lat_lng_list)>=2:
            lat = lat_lng_list[0]
            lng = lat_lng_list[1]
        print("Latitude = {}, Longitude = {}".format(lat, lng))

except Exception as e:
        print("Error: ", str(e))


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# # 6. Write a program to scrap all the available details of best gaming laptops from digit.in

# In[ ]:


driver=webdriver.Chrome()


# In[ ]:


driver.get("https://www.digit.in/top-products/best-gaming-laptops-40.html")


# In[ ]:


Brands=[]
Products_Description=[]
Specification=[]
Price=[]


# In[ ]:


brand=driver.find_elements(By.XPATH,"//div[@class='rh_gr_top_middle mb10 colored_rate_bar']")

len(brand)


# In[ ]:


for i in brand:
   
    Brands.append(str(i.text).replace("\n",""))
Brands


# In[ ]:


driver.get("https://www.digit.in/laptops/hp-omen-17-ck2008tx-13th-gen-core-i7-13700hx-price-346659.html")


# # 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: 
# “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”

# In[176]:


driver=webdriver.Chrome()


# In[177]:


driver.get("https://www.forbes.com/billionaires/")


# In[178]:


Rank=[]
Name=[]
Net_worth=[]
Age=[]
Citizenship=[]
Source=[]
Industry=[]


# In[179]:


rank=driver.find_elements(By.XPATH,'//div[@class="Table_rank___YBhk Table_dataCell__2QCve"]')
for i in rank:
    rnk=i.text
    Rank.append(rnk)
    
name=driver.find_elements(By.XPATH,'//div[@class="Table_dataCell__2QCve"]')
for i in name:
    nme=i.text
    Name.append(nme)
    
net_worth=driver.find_elements(By.XPATH,'//div[@class="Table_netWorth___L4R5 Table_dataCell__2QCve"]')
for i in net_worth:
    net=i.text
    Net_worth.append(net)
    
age=driver.find_elements(By.XPATH,'//div[@class="Table_dataCell__2QCve"]')
for i in age:
    age=i.text
    Age.append(age)
    
    
citizenship=driver.find_elements(By.XPATH,' //div[@class="TableRow_cell__db-hv Table_cell__houv9"]')
for i in citizenship:
    net=i.text
    Citizenship.append(net)
    
source=driver.find_elements(By.XPATH,'//div[@class="Table_dataCell__2QCve"]')
for i in source:
    source=i.text
    Source.append(source)
    
    
industry=driver.find_elements(By.XPATH,'//div[@class="Table_dataCell__2QCve"]')
for i in industry:
    industry=i.text
    Industry.append(industry)
    

print(len(Rank),len(Name),len(Net_worth),len(Age),len(Citizenship),len(Source),len(Industry))


# # 8.Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted 
# from any YouTube Video

# In[26]:


driver=webdriver.Chrome()


# In[27]:


driver.get('https://www.youtube.com/')


# In[31]:


search=driver.find_elements(By.ID,'//div[id="search-input"')
search.send_keys(Data_science)

#search_element = driver.find_element(By.XPATH, "//*[@id='twotabsearchtextbox']" )
#search_element.send_keys(Input_element)


# In[ ]:





# In[ ]:





# # 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in 
# “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall 
# reviews, privates from price, dorms from price, facilities and property description

# In[ ]:





# In[ ]:





# In[ ]:


driver=webdriver.Chrome()


# In[ ]:


driver.get("https://www.hostelworld.com/ ")


# In[ ]:


location = driver.find_element(By.CLASS_NAME, "native-input" )
location.send_keys('London')


# In[ ]:


search_button=driver.find_element(By.CLASS_NAME, "btn-content medium-button icon-only")
search_button.click()


# In[ ]:




